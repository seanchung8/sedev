{
 "metadata": {
  "name": "",
  "signature": "sha256:aaaefb71d96e30a599c4d10c6aaac4bb63cb89397f976089d003506c73c8f9d9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Science Demo - Hortonworks"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling airline delays with Hadoop and Spark using Scala"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This demo is the 2nd part of the airline delay modeling exercise, where we will demonstrate how to use Spark for pre-processing and for modeling (using ML-Lib).\n",
      "\n",
      "In this demo we will use Spark's Scala API.\n",
      "\n",
      "As a reminder, we are exploring the airline delay dataset available here:  http://stat-computing.org/dataexpo/2009/the-data.html, which includes details about flights in the US, and specifically we are using the years 2007 and 2008."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pre-processing with Spark and Scala"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import date\n",
      "holidays = [\n",
      "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
      "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n",
      "        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n",
      "        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n",
      "     ]\n",
      "\n",
      "def get_hour(val): return(int(val.zfill(4)[:2]))\n",
      "def days_from_nearest_holiday(year, month, day):\n",
      "  d = date(year, month, day)\n",
      "  x = [(abs(d-h)).days for h in holidays]\n",
      "  return min(x)\n",
      "def is_bad_carrier(carrier): return (1 if (carrier in ['UA', 'MQ', 'AA']) else 0)\n",
      "def is_bad_dest(dest): return (1 if (dest in ['LGA', 'EWR', 'MSP', 'DFW', 'LAX']) else 0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Iteration 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "\n",
      "fields = [\"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \n",
      "          \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \n",
      "          \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \n",
      "          \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"]\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[fields.index('DepTime')])]\n",
      "    f3 = [row[fields.index('Distance')]]\n",
      "    year = int(row[fields.index('Year')])\n",
      "    month = int(row[fields.index('Month')])\n",
      "    day = int(row[fields.index('Day')])\n",
      "    f4 = [is_bad_carrier(row[fields.index('Carrier')]), is_bad_dest(row[fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return ','.join([str(x) for x in flist])\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(infile):\n",
      "  lines = sc.textFile(infile)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[fields.index('Origin')] == 'ORD')\n",
      "  results = data.map(gen_features)\n",
      "  return results\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_1'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_1'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Iteration 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##%%time \n",
      "\n",
      "delay_fields = [ \"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \\\n",
      "            \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \\\n",
      "            \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \\\n",
      "            \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\" ]\n",
      "\n",
      "year_inx = delay_fields.index('Year')\n",
      "month_inx = delay_fields.index('Month')\n",
      "day_inx = delay_fields.index('Day')\n",
      "\n",
      "weather_fields = [ \"station\", \"date\", \"metric\", \"value\", \"t1\", \"t2\", \"t3\", \"t4\" ]\n",
      "\n",
      "date_inx = weather_fields.index('date')\n",
      "metric_inx = weather_fields.index('metric')\n",
      "value_inx = weather_fields.index('value')\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[delay_fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[delay_fields.index('DepTime')])]\n",
      "    f3 = [row[delay_fields.index('Distance')]]\n",
      "    year = int(row[year_inx])\n",
      "    month = int(row[month_inx])\n",
      "    day = int(row[day_inx])\n",
      "    f4 = [is_bad_carrier(row[delay_fields.index('Carrier')]), is_bad_dest(row[delay_fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return (to_date(year, month, day), flist)\n",
      "\n",
      "def to_date(year, month, day):\n",
      "  s = \"%04d%02d%02d\" % (year, month, day)\n",
      "  return s\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(delay_file, weather_file):\n",
      "  \n",
      "  # Read airline delay dataset\n",
      "  lines = sc.textFile(delay_file)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Origin')] == 'ORD')\n",
      "  res_delay = data.map(gen_features)\n",
      "                \n",
      "  # Read weather data into RDDs\n",
      "  lines = sc.textFile(weather_file)\n",
      "  data = lines.map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[weather_fields.index('station')] == 'USW00094846')\n",
      "  w_tmin = data.filter(lambda vals: vals[metric_inx] == 'TMIN') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_tmax = data.filter(lambda vals: vals[metric_inx] == 'TMAX') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_prcp = data.filter(lambda vals: vals[metric_inx] == 'PRCP') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_snow = data.filter(lambda vals: vals[metric_inx] == 'SNOW') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_awnd = data.filter(lambda vals: vals[metric_inx] == 'AWND') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "                    \n",
      "  # Join weather data with delay data\n",
      "  joined = res_delay.join(w_tmin).join(w_tmax).join(w_prcp).join(w_snow).join(w_awnd) \\\n",
      "           .map(lambda pair: ','.join([str(x) for x in pair[1]]))     \n",
      "\n",
      "  return joined\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv', 'airline/weather/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv', 'airline/weather/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "<console>:21: error: inferred type arguments [Array[String]] do not conform to method createSchemaRDD's type parameter bounds [A <: Product]",
        "              val flights = sqlContext.createSchemaRDD(flightData)",
        "                                       ^",
        "<console>:21: error: type mismatch;",
        " found   : org.apache.spark.rdd.RDD[Array[String]]",
        " required: org.apache.spark.rdd.RDD[A]",
        "              val flights = sqlContext.createSchemaRDD(flightData)",
        "                                                       ^"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": []
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": []
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling with Spark and ML-Lib: Logistic regression and SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(',')]\n",
      "    return LabeledPoint(1 if values[0]>=15 else 0, [float(x) for x in values[1:]])\n",
      "\n",
      "# Evaluating the model's performance\n",
      "def eval_metrics(labelsAndPreds):\n",
      "    tp = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==1).count())\n",
      "    tn = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==0).count())\n",
      "    fp = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==1).count())\n",
      "    fn = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==0).count())\n",
      "\n",
      "    precision = tp / (tp+fp)\n",
      "    recall = tp / (tp+fn)\n",
      "    F_measure = 2*precision*recall / (precision+recall)\n",
      "    return [precision, recall, F_measure]\n",
      "\n",
      "\n",
      "# Build the model\n",
      "train_data = sc.textFile('airline/fm/ord_2007_2')\n",
      "parsedTrainData = train_data.map(parsePoint)\n",
      "model_lr = LogisticRegressionWithSGD.train(parsedTrainData, iterations=100)\n",
      "\n",
      "# Predict\n",
      "test_data = sc.textFile('airline/fm/ord_2008_2')\n",
      "parsedTestData = test_data.map(parsePoint)\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_lr.predict(p.features)))\n",
      "\n",
      "print eval_metrics(labelsAndPreds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.3648417499913901, 0.6660170166394233, 0.4714337632670014]\n",
        "CPU times: user 149 ms, sys: 54 ms, total: 203 ms\n",
        "Wall time: 53.8 s\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's try with SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import SVMWithSGD\n",
      "\n",
      "# Building the SVM model\n",
      "model_svm = SVMWithSGD.train(parsedTrainData, iterations=200)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_svm.predict(p.features)))\n",
      "print eval_metrics(labelsAndPreds)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.28924389448874177, 0.9669831090992916, 0.4452920683632012]\n",
        "CPU times: user 189 ms, sys: 82 ms, total: 271 ms\n",
        "Wall time: 53.3 s\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}