{
 "metadata": {
  "name": "",
  "signature": "sha256:c485ca9270bcc5a19fc4363132972b6b974812366a4e74d53ba4a33f081fca1e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Science Demo"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a predictive model for airline delays - with Hadoop and PySpark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this second blog post, we continue to demonstrate how to build a predictive model with Hadoop, this time we'll use Spark and ML-Lib.\n",
      "\n",
      "Apache Spark is a relatively new entrant to the Hadoop ecosystem. Running on YARN, Apache Spark is an in-memory data processing API and execution engine that is effective for machine learning and data science use cases along side other workloads.\n",
      "\n",
      "In the context of our demo, we will show how to use Apache Spark via its PySpark API to generate our feature matrix and also use ML-Lib (Spark's machine learning library) to build and evaluate models.\n",
      "\n",
      "Recall from part 1 that we are exploring a predictive model for flight delays. Our source dataset resides here: http://stat-computing.org/dataexpo/2009/the-data.html, and includes details about flights in the US from the years 1987-2008. We have also enriched the data with weather information from: http://www.ncdc.noaa.gov/cdo-web/datasets/, where we find daily temperatures (min/max), wind speed, snow conditions and precipitation. \n",
      "\n",
      "We will build a supervised learning model to predict flight delays for flights leaving O'Hare International airport (ORD). We will use the year 2007 data to build the model, and test it's validity using data from 2008.\n",
      "\n",
      "So let's being"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pre-processing with Python and PySpark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apache Spark's basic data abstraction is that of an RDD (resilient distributed dataset), which is a fault-tolerant collection of elements that can be operated on in parallel across your Hadoop cluster. \n",
      "\n",
      "Spark's API (available in Scala, Python or Java) supports a variety of operations such as map() and flatMap(), filter(), join(), and more. For a full description of the API please check the Spark API programming guide: http://spark.apache.org/docs/1.1.0/programming-guide.html \n",
      "\n",
      "We will show how to perform the same pre-processing with Spark (using its Python API - PySpark) as we did with PIG previously. First, let's define a few Python functions for our features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import date\n",
      "holidays = [\n",
      "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
      "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n",
      "        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n",
      "        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n",
      "     ]\n",
      "\n",
      "def get_hour(val): return(int(val.zfill(4)[:2]))\n",
      "def days_from_nearest_holiday(year, month, day):\n",
      "  d = date(year, month, day)\n",
      "  x = [(abs(d-h)).days for h in holidays]\n",
      "  return min(x)\n",
      "def is_bad_carrier(carrier): return (1 if (carrier in ['UA', 'MQ', 'AA']) else 0)\n",
      "def is_bad_dest(dest): return (1 if (dest in ['LGA', 'EWR', 'MSP', 'DFW', 'LAX']) else 0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we use Spark to create the simple feature matrices for iteration 1. \n",
      "\n",
      "The python function gen_features(row) takes a row of input and generates a comma-separated string with all the features.\n",
      "preprocess_spark() performs the complete pre-processing task using Spark on a given file. In our case we need to do this for both the 2007 file (our training set) and 2008 file (our validation/test set).\n",
      "\n",
      "We store the resulting feature matrices under the \"airline/fm-spark\" folder."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "\n",
      "fields = [\"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \n",
      "          \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \n",
      "          \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \n",
      "          \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"]\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[fields.index('DepTime')])]\n",
      "    f3 = [row[fields.index('Distance')]]\n",
      "    year = int(row[fields.index('Year')])\n",
      "    month = int(row[fields.index('Month')])\n",
      "    day = int(row[fields.index('Day')])\n",
      "    f4 = [is_bad_carrier(row[fields.index('Carrier')]), is_bad_dest(row[fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return ','.join([str(x) for x in flist])\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(infile):\n",
      "  lines = sc.textFile(infile)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[fields.index('Origin')] == 'ORD')\n",
      "  results = data.map(gen_features)\n",
      "  return results\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_1'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_1'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 3.49 s, sys: 239 ms, total: 3.73 s\n",
        "Wall time: 1min 23s\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pre-processing: Iteration 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that in part 1, we decided to enrich the dataset by integrating weather, and we've seen that modeling produces better results with the additional features. Let's implement this enhanced feature matrix generation as well here, again using Apache Spark.\n",
      "\n",
      "Here we need to join the two datasets, by the date field. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "\n",
      "delay_fields = [ \"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \\\n",
      "            \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \\\n",
      "            \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \\\n",
      "            \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\" ]\n",
      "\n",
      "year_inx = delay_fields.index('Year')\n",
      "month_inx = delay_fields.index('Month')\n",
      "day_inx = delay_fields.index('Day')\n",
      "\n",
      "weather_fields = [ \"station\", \"date\", \"metric\", \"value\", \"t1\", \"t2\", \"t3\", \"t4\" ]\n",
      "\n",
      "date_inx = weather_fields.index('date')\n",
      "metric_inx = weather_fields.index('metric')\n",
      "value_inx = weather_fields.index('value')\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[delay_fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[delay_fields.index('DepTime')])]\n",
      "    f3 = [row[delay_fields.index('Distance')]]\n",
      "    year = int(row[year_inx])\n",
      "    month = int(row[month_inx])\n",
      "    day = int(row[day_inx])\n",
      "    f4 = [is_bad_carrier(row[delay_fields.index('Carrier')]), is_bad_dest(row[delay_fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return (to_date(year, month, day), flist)\n",
      "\n",
      "def to_date(year, month, day):\n",
      "  s = \"%04d%02d%02d\" % (year, month, day)\n",
      "  return s\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(delay_file, weather_file):\n",
      "  \n",
      "  # Read airline delay dataset\n",
      "  lines = sc.textFile(delay_file)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Origin')] == 'ORD')\n",
      "  res_delay = data.map(gen_features)\n",
      "                \n",
      "  # Read weather data into RDDs\n",
      "  lines = sc.textFile(weather_file)\n",
      "  data = lines.map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[weather_fields.index('station')] == 'USW00094846')\n",
      "  w_tmin = data.filter(lambda vals: vals[metric_inx] == 'TMIN') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_tmax = data.filter(lambda vals: vals[metric_inx] == 'TMAX') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_prcp = data.filter(lambda vals: vals[metric_inx] == 'PRCP') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_snow = data.filter(lambda vals: vals[metric_inx] == 'SNOW') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_awnd = data.filter(lambda vals: vals[metric_inx] == 'AWND') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "                    \n",
      "  # Join weather data with delay data\n",
      "  joined = res_delay.join(w_tmin).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_tmax).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_prcp).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_snow).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_awnd).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .map(lambda pair: ','.join([str(x) for x in pair[1]]))     \n",
      "\n",
      "  return joined\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv', 'airline/weather/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "print \"Finished pre-processing for 2007 datasets\"\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv', 'airline/weather/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n",
      "print \"Finished pre-processing for 2008 datasets\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling with Spark and ML-Lib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the training and validation datasets ready, let's see how to build a predictive model with Spark's ML-Lib machine learning library.\n",
      "\n",
      "MLlib is Spark\u2019s scalable machine learning library, which includes various learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, and others. \n",
      "\n",
      "If you compare ML-Lib to Scikit-learn, at the moment ML-Lib lacks a few important algorithms like Random Forest, Gradient Boosted Trees, and others. But nonetheless it's a strong library with a bright future. \n",
      "\n",
      "Let's run a few predictive models with ML-Lib. First we parse our feature matrices into RDDs of LabeledPoint instances for both the training and test datasets. We also define \"Eval_metrics\" a helper function to compute precision, recall, F1 measure and accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.regression import LabeledPoint\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(',')]\n",
      "    return LabeledPoint(1 if values[0]>=15 else 0, [float(x) for x in values[1:]])\n",
      "\n",
      "# Evaluating the model's performance\n",
      "def eval_metrics(labelsAndPreds):\n",
      "    tp = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==1).count())\n",
      "    tn = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==0).count())\n",
      "    fp = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==1).count())\n",
      "    fn = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==0).count())\n",
      "\n",
      "    precision = tp / (tp+fp)\n",
      "    recall = tp / (tp+fn)\n",
      "    F_measure = 2*precision*recall / (precision+recall)\n",
      "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
      "    return {'precision': precision, 'recall': recall, 'F1': F_measure, 'accuracy': accuracy}\n",
      "\n",
      "# Prepare training set\n",
      "train_data = sc.textFile('airline/fm-spark/ord_2007_2')\n",
      "parsedTrainData = train_data.map(parsePoint)\n",
      "\n",
      "# Prepare test set\n",
      "test_data = sc.textFile('airline/fm-spark/ord_2008_2')\n",
      "parsedTestData = test_data.map(parsePoint)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ML-Lib supports linear regression and its classification variants, implemented via Stochastic Gradient descent (SGD).\n",
      "Let's see how to use it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "\n",
      "# Build the LR model\n",
      "model_lr = LogisticRegressionWithSGD.train(parsedTrainData, iterations=100)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_lr.predict(p.features)))\n",
      "m = eval_metrics(labelsAndPreds)\n",
      "print (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m['precision'], m['recall'], m['F1'], m['accuracy']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "precision = 0.35, recall= 0.55, F1 = 0.43, accuracy = 0.59\n",
        "CPU times: user 515 ms, sys: 216 ms, total: 731 ms\n",
        "Wall time: 1min 2s\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now let's try the Support Vector Machine version of ML-Lib:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import SVMWithSGD\n",
      "\n",
      "# Build the SVM model\n",
      "model_svm = SVMWithSGD.train(parsedTrainData, iterations=200)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_svm.predict(p.features)))\n",
      "m = eval_metrics(labelsAndPreds)\n",
      "print (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m['precision'], m['recall'], m['F1'], m['accuracy']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "precision = 0.28, recall= 1.00, F1 = 0.44, accuracy = 0.28\n",
        "CPU times: user 876 ms, sys: 393 ms, total: 1.27 s\n",
        "Wall time: 1min 21s\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}