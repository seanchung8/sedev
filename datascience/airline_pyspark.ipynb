{
 "metadata": {
  "name": "",
  "signature": "sha256:51690fbd9eebdf094094b1aed28ae1709b5e50cb048f84f25b8c53df57d431f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Science Demo - Hortonworks"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling airline delays with Hadoop and PySpark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This demo is the 2nd part of the airline delay modeling exercise, where we will demonstrate how to use Spark for pre-processing and for modeling (using ML-Lib).\n",
      "\n",
      "In this demo we will use PySpark, Spark's Python API.\n",
      "\n",
      "As a reminder, we are exploring the airline delay dataset available here:  http://stat-computing.org/dataexpo/2009/the-data.html, which includes details about flights in the US, and specifically we are using the years 2007 and 2008."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "<pyspark.context.SparkContext at 0x7f9df4bfe2d0>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "IPython.OutputArea.auto_scroll_threshold = 30;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "IPython.OutputArea.auto_scroll_threshold = 30;"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x7f9df4bafa90>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function to read HDFS file into dataframe using PyDoop\n",
      "\n",
      "import pydoop.hdfs as hdfs\n",
      "def read_csv_from_hdfs(path, cols, col_types=None):\n",
      "  files = hdfs.ls(path);\n",
      "  pieces = []\n",
      "  for f in files:\n",
      "    pieces.append(pd.read_csv(hdfs.open(f), names=cols, dtype=col_types))\n",
      "  return pd.concat(pieces, ignore_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pre-processing with Python and PySpark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For pre-processing, Spark provides many of the same constructs and operators as do PIG, HIVE or Cascading. We will now show how to implement the same pre-processing with SPARK via the PySpark API. First, we define some python functions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import date\n",
      "holidays = [\n",
      "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
      "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n",
      "        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n",
      "        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n",
      "     ]\n",
      "\n",
      "def get_hour(val): return(int(val.zfill(4)[:2]))\n",
      "def days_from_nearest_holiday(year, month, day):\n",
      "  d = date(year, month, day)\n",
      "  x = [(abs(d-h)).days for h in holidays]\n",
      "  return min(x)\n",
      "def is_bad_carrier(carrier): return (1 if (carrier in ['UA', 'MQ', 'AA']) else 0)\n",
      "def is_bad_dest(dest): return (1 if (dest in ['LGA', 'EWR', 'MSP', 'DFW', 'LAX']) else 0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we use Spark to create the feature matrices from iteration 1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "\n",
      "fields = [\"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \n",
      "          \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \n",
      "          \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \n",
      "          \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"]\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[fields.index('DepTime')])]\n",
      "    f3 = [row[fields.index('Distance')]]\n",
      "    year = int(row[fields.index('Year')])\n",
      "    month = int(row[fields.index('Month')])\n",
      "    day = int(row[fields.index('Day')])\n",
      "    f4 = [is_bad_carrier(row[fields.index('Carrier')]), is_bad_dest(row[fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return ','.join([str(x) for x in flist])\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(infile):\n",
      "  lines = sc.textFile(infile)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[fields.index('Origin')] == 'ORD')\n",
      "  results = data.map(gen_features)\n",
      "  return results\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_1'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_1'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 3.69 s, sys: 180 ms, total: 3.87 s\n",
        "Wall time: 1min 20s\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Iteration 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's see how to integrate the weather data using SPARK, same as we did in iteration 2 with pig:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "\n",
      "delay_fields = [ \"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \\\n",
      "            \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \\\n",
      "            \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \\\n",
      "            \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\" ]\n",
      "\n",
      "year_inx = delay_fields.index('Year')\n",
      "month_inx = delay_fields.index('Month')\n",
      "day_inx = delay_fields.index('Day')\n",
      "\n",
      "weather_fields = [ \"station\", \"date\", \"metric\", \"value\", \"t1\", \"t2\", \"t3\", \"t4\" ]\n",
      "\n",
      "date_inx = weather_fields.index('date')\n",
      "metric_inx = weather_fields.index('metric')\n",
      "value_inx = weather_fields.index('value')\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[delay_fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[delay_fields.index('DepTime')])]\n",
      "    f3 = [row[delay_fields.index('Distance')]]\n",
      "    year = int(row[year_inx])\n",
      "    month = int(row[month_inx])\n",
      "    day = int(row[day_inx])\n",
      "    f4 = [is_bad_carrier(row[delay_fields.index('Carrier')]), is_bad_dest(row[delay_fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return (to_date(year, month, day), flist)\n",
      "\n",
      "def to_date(year, month, day):\n",
      "  s = \"%04d%02d%02d\" % (year, month, day)\n",
      "  return s\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(delay_file, weather_file):\n",
      "  \n",
      "  # Read airline delay dataset\n",
      "  lines = sc.textFile(delay_file)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Origin')] == 'ORD')\n",
      "  res_delay = data.map(gen_features)\n",
      "                \n",
      "  # Read weather data into RDDs\n",
      "  lines = sc.textFile(weather_file)\n",
      "  data = lines.map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[weather_fields.index('station')] == 'USW00094846')\n",
      "  w_tmin = data.filter(lambda vals: vals[metric_inx] == 'TMIN') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_tmax = data.filter(lambda vals: vals[metric_inx] == 'TMAX') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_prcp = data.filter(lambda vals: vals[metric_inx] == 'PRCP') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_snow = data.filter(lambda vals: vals[metric_inx] == 'SNOW') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_awnd = data.filter(lambda vals: vals[metric_inx] == 'AWND') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "                    \n",
      "  # Join weather data with delay data\n",
      "  joined = res_delay.join(w_tmin).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_tmax).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_prcp).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_snow).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_awnd).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .map(lambda pair: ','.join([str(x) for x in pair[1]]))     \n",
      "\n",
      "  return joined\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv', 'airline/weather/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "print \"Finished pre-processing for 2007 datasets\"\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv', 'airline/weather/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n",
      "print \"Finished pre-processing for 2008 datasets\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling with Spark and ML-Lib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we build a Logistic Regression model with Spark and ML-Lib. We will only run the models on the output of iteration 2 files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(',')]\n",
      "    return LabeledPoint(1 if values[0]>=15 else 0, [float(x) for x in values[1:]])\n",
      "\n",
      "# Evaluating the model's performance\n",
      "def eval_metrics(labelsAndPreds):\n",
      "    tp = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==1).count())\n",
      "    tn = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==0).count())\n",
      "    fp = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==1).count())\n",
      "    fn = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==0).count())\n",
      "\n",
      "    precision = tp / (tp+fp)\n",
      "    recall = tp / (tp+fn)\n",
      "    F_measure = 2*precision*recall / (precision+recall)\n",
      "    return [precision, recall, F_measure]\n",
      "\n",
      "\n",
      "# Build the model\n",
      "train_data = sc.textFile('airline/fm-spark/ord_2007_2')\n",
      "parsedTrainData = train_data.map(parsePoint)\n",
      "model_lr = LogisticRegressionWithSGD.train(parsedTrainData, iterations=100)\n",
      "\n",
      "# Predict\n",
      "test_data = sc.textFile('airline/fm-spark/ord_2008_2')\n",
      "parsedTestData = test_data.map(parsePoint)\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_lr.predict(p.features)))\n",
      "\n",
      "print eval_metrics(labelsAndPreds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.3523417319213968, 0.5459679785405926, 0.4282872619370535]\n",
        "CPU times: user 620 ms, sys: 239 ms, total: 859 ms\n",
        "Wall time: 1min 15s\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's try the SVM with SGD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import SVMWithSGD\n",
      "\n",
      "# Building the SVM model\n",
      "model_svm = SVMWithSGD.train(parsedTrainData, iterations=200)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_svm.predict(p.features)))\n",
      "print eval_metrics(labelsAndPreds)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.2846032266722333, 1.0, 0.4430990375284957]\n",
        "CPU times: user 1.01 s, sys: 457 ms, total: 1.47 s\n",
        "Wall time: 1min 53s\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}